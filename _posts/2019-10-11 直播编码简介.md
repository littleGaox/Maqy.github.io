---
layout:     post
title:      直播编码简介
subtitle:   
date:       2019-10-11
author:     Maqy
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Live

---



## 视频基础知识简介

视频采集工作一般用 AVFoundation 库来完成，这里涉及到一些基础类型简单介绍下：

CMSampleBuffer: 存放编解码前后的视频图像的容器数据结构

CVPixelBuffer:  编码前和解码后的图像数据结构

CMBlockBuffer: 编码后图像的数据结构

pixelBufferAttributes: : CFDictionary对象，可能包含了视频的宽高，像素格式类型（32RGBA, YCbCr420），是否可以用于OpenGL ES等相关信息

CMTime: 时间戳相关。用来校对音视频，且和播放速度相关



视频采集一般用AVFoundation框架来实现

而h264 h265编码一般用VideoToolbox框架来实现



## 编码概览

#### 1.1 原因

视频数据原始体积巨大,以720P 30fps的视频为例,一个像素大约3个字节,如下所得,每秒钟产生87MB,这样计算可得一分钟就将产生5.22GB.

```
数据量/每秒=1280*720*33*3/1024/1024=87MB
```

视频是按帧来显示的，每一帧都是一张图像，那么我们可以根据一些图像压缩算法来进行压缩，也可以根据视频前后帧的相似度进行压缩，从而将视频大小压缩。

#### 1.2软硬编码

软件编码：使用CPU进行编码。

硬件编码：不使用CPU进行编码，使用显卡GPU,专用的DSP、FPGA、ASIC芯片等硬件进行编码。

优缺点

软编：实现直接、简单，参数调整方便，升级易，但CPU负载重，性能较硬编码低，低码率下质量通常比硬编码要好一点。

硬编：性能高，低码率下通常质量低于硬编码器，但部分产品在GPU硬件平台移植了优秀的软编码算法（如X264）的，质量基本等同于软编码。

#### 1.3 h264 h265编码原理

这里只简单阐述，复杂的原理没必要理解哈

视频采集是以帧的形式作为回调，每秒会返回固定帧数的画面，编码过程主要就是讲不通的视频帧作区分，并分别对待进行压缩，主要的分类如下：

- I帧:关键帧.完整编码的帧.可以理解成是一张完整画面,不依赖其他帧
- P帧:参考前面的I帧或P帧,即通过前面的I帧与自己记录的不同的部分可以形成完整的画面.因此,单独的P帧无法形成画面.
- B帧:参考前面的I帧或P帧以及后面的P帧



压缩算法主要分两种：帧内压缩和帧间压缩

- 帧内压缩

当压缩一帧图像时，仅考虑本帧的数据而不考虑相邻帧之间的冗余信息，这实际上与静态图像压缩类似。帧内一般采用有损压缩算法，由于帧内压缩是编码一个完整的图像，所以可以独立的解码、显示。帧内压缩一般达不到很高的压缩，跟编码jpeg差不多。（其实就是压缩图像的算法）

- 帧间压缩: P帧与B帧的压缩算法

相邻几帧的数据有很大的相关性，或者说前后两帧信息变化很小的特点。也即连续的视频其相邻帧之间具有冗余信息,根据这一特性，压缩相邻帧之间的冗余量就可以进一步提高压缩量，减小压缩比。帧间压缩也称为时间压缩（Temporal compression），它通过比较时间轴上不同帧之间的数据进行压缩。帧间压缩一般是无损的。帧差值（Frame differencing）算法是一种典型的时间压缩法，它通过比较本帧与相邻帧之间的差异，仅记录本帧与其相邻帧的差值，这样可以大大减少数据量。



解压算法：DTS和PTS

- DTS:主要用于视频的解码,在解码阶段使用.
- PTS:主要用于视频的同步和输出.在渲染的时候使用.在没有B frame的情况下.DTS和PTS的输出顺序是一样的。



## iOS中的应用

#### 采集

采集视频可以用AVFoundation框架，AVCaptureSession用来初始化一个视频采集的session，配置好参数后即可通过回调拿到视频的帧数据CMSampleBuffer

```
		AVCaptureSession *session = [[AVCaptureSession alloc] init];
    // 设置分辨率
    session.sessionPreset = AVCaptureSessionPreset1920x1080;
    
    // 设置输入源：摄像头
    AVCaptureDevice *device = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];
    AVCaptureDeviceInput *input = [AVCaptureDeviceInput deviceInputWithDevice:device error:&error];
    [session addInput:input];
    
    // 设置输出源
    _video_output = [[AVCaptureVideoDataOutput alloc] init];
    ...
    [session addOutput:_video_output];
    
    // 显示
    ...
    AVCaptureVideoPreviewLayer *newPreviewLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession:session];
    ...
```



VideoToolbox 框架，你可以通过直接访问硬编解码器，将 H.264 文件或传输流转换为 iOS 上的 CMSampleBuffer 并解码成 CVPixelBuffer，或将未压缩的 CVPixelBuffer 编码成 CMSampleBuffer

采集音频可以用Audio Unit，音视频校对是通过CMTime时间戳来进行的。



#### 渲染

将 CMSampleBuffer渲染到界面显示的方式有两种：

- 将CMSampleBuffers提供给系统的AVSampleBufferDisplayLayer 直接显示
  - 使用方式和其它CALayer类似。该层内置了硬件解码功能，将原始的CMSampleBuffer解码后的图像直接显示在屏幕上面，非常的简单方便。
- 利用OPenGL自己渲染 通过VTDecompression接口来，将CMSampleBuffer解码成图像，将图像通过UIImageView或者OpenGL上显示。



#### 推拉流流程

1.视频采集，视频帧编码成h264码流

2.音频采集，音频数据编码

3.将编码后的音视频通过时间戳校对

4.合成flv混合文件

5.rtmp传输

6.rtmp接收

7.解码

8.渲染

